# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IhCIlL7QHu9cN3yezOLYDvyE5VuKK_wN
"""

from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Step 1: Login to Hugging Face
login("hugging face token")  # Replace  Hugging Face token

# Step 2: Load the IBM Granite model
model_id = "ibm-granite/granite-3.3-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
# Create text generation pipeline
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Step 4: Define the assistant function
def health_ai(prompt):
    result = pipe(prompt, max_new_tokens=300, do_sample=True, temperature=0.7)
    return result[0]["generated_text"]

# Step 5: Example usage
print(health_ai("I have fever and body pain. What might be the problem and what should I do?"))